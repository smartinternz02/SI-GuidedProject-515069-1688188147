{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WqSelrVeIwqOpmgIVBteDRvu4VJU64tV",
      "authorship_tag": "ABX9TyNUUHDCNRFGat1B0OEVndCD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartinternz02/SI-GuidedProject-515069-1688188147/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set the path to the PlantDoc dataset\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/PlantDoc-Dataset'\n",
        "\n",
        "# Set the image size for resizing\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Set the batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Set the number of training samples\n",
        "train_samples = 8000\n",
        "\n",
        "# Set the number of epochs for training\n",
        "epochs = 10\n",
        "\n",
        "# Update the data augmentation and normalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load the entire dataset with augmented data\n",
        "data_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "# Calculate the number of validation samples\n",
        "validation_samples = data_generator.samples - train_samples\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_generator = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    data_generator.directory,\n",
        "    data_generator.image_data_generator,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='training')\n",
        "\n",
        "validation_generator = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    data_generator.directory,\n",
        "    data_generator.image_data_generator,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='validation')\n",
        "\n",
        "# Set the number of layers to fine-tune\n",
        "fine_tune_layers = 10\n",
        "\n",
        "# Unfreeze the last `fine_tune_layers` layers for training\n",
        "for layer in model.layers[-fine_tune_layers:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Create a new model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model again after modifying the layers\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Define a custom learning rate\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model with the custom learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_samples // batch_size)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('plant_disease_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcrrI4GzSFor",
        "outputId": "f0635e5b-ea75-4438-efa0-a149a6c54587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2552 images belonging to 2 classes.\n",
            "Found 2552 images belonging to 2 classes.\n",
            "Found 0 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            " 80/250 [========>.....................] - ETA: 53:06 - loss: 0.3590 - accuracy: 0.8973"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2500 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r250/250 [==============================] - 1504s 6s/step - loss: 0.3590 - accuracy: 0.8973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drn71makRwWu",
        "outputId": "043f4c19-8dd4-4876-e099-ffeadc3e10f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 613ms/step\n",
            "The predicted label for the input image is: healthy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('plant_disease_model.h5')\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = ['disease', 'healthy']\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Load the image from the given path\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    # Expand the dimensions to match the model input requirements\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocess the image (e.g., mean subtraction)\n",
        "    preprocessed_img = preprocess_input(img_array)\n",
        "\n",
        "    return preprocessed_img\n",
        "\n",
        "def predict_disease(image_path):\n",
        "    # Preprocess the image\n",
        "    preprocessed_img = preprocess_image(image_path)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(preprocessed_img)\n",
        "\n",
        "    # Get the predicted class label\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage:\n",
        "image_path = '/content/Tomato leaf late blight (1).jpg'  # Replace with the actual image path\n",
        "predicted_label = predict_disease(image_path)\n",
        "print(f\"The predicted label for the input image is: {predicted_label}\")"
      ]
    }
  ]
}